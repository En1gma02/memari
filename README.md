<p align="center">
  <h1 align="center">ğŸ§  Memari</h1>
  <p align="center"><strong>An AI Chatbot with Infinite Long-Term Memory</strong></p>
  <p align="center">
    <em>Building human-like memory for AI conversations</em>
  </p>
</p>

---

## ğŸ¯ Vision

**Memari** is an AI chatbot named **Ari** designed to remember conversations like a human friend would. Unlike traditional chatbots that forget context after a session ends, Ari can recall references to events discussed weeks, months, or even years ago.

> *"Talking to Ari should feel like chatting with a friend on WhatsApp who actually remembers your life."*

This project demonstrates a novel **Hybrid Memory Architecture** for solving the infinite memory problem in conversational AI.

---

## ğŸ—ï¸ Architecture

Memari implements a **3-Layer Hybrid Memory System**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MEMORY ARCHITECTURE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  SHORT-TERM      â”‚  â”‚  USER PERSONA    â”‚  â”‚  LONG-TERM     â”‚ â”‚
â”‚  â”‚  MEMORY          â”‚  â”‚  MEMORY          â”‚  â”‚  MEMORY        â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Last 5-10 msgs   â”‚  â”‚ Structured MD    â”‚  â”‚ FAISS Vector   â”‚ â”‚
â”‚  â”‚ in context       â”‚  â”‚ file with user   â”‚  â”‚ Database with  â”‚ â”‚
â”‚  â”‚ window           â”‚  â”‚ facts, likes,    â”‚  â”‚ session-based  â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚ personality      â”‚  â”‚ embeddings     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â–²                      â–²                     â–²          â”‚
â”‚         â”‚                      â”‚                     â”‚          â”‚
â”‚    Always Active          Tool Call            Tool Call        â”‚
â”‚                        get_user_persona    get_long_term_memory â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Layers

| Layer | Purpose | Storage | Retrieval |
|-------|---------|---------|-----------|
| **Short-Term** | Immediate conversation context | In-memory buffer | Always included |
| **User Persona** | Facts, preferences, personality | Markdown file | Tool call |
| **Long-Term** | Historical conversations | FAISS + Embeddings | RAG Pipeline |

---

## ğŸ”§ Tech Stack

| Component | Technology |
|-----------|------------|
| **Frontend** | Next.js 16 + React 19 + shadcn/ui + Tailwind v4 |
| **Backend** | FastAPI (Python) |
| **Vector DB** | FAISS (faiss-cpu) |
| **Embeddings** | sentence-transformers (`all-MiniLM-L6-v2`) |
| **Re-ranking** | CrossEncoder (`ms-marco-MiniLM-L-2-v2`) |
| **LLM - Fast** | Cerebras (`llama-3.1-8b`) |
| **LLM - Smart** | Groq (`gpt-oss-120b` with tool calling) |
| **Safety** | Groq (`llama-guard-4-12b`) |

---

## ğŸ“ Project Structure

```
memari/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py            # FastAPI endpoints (minimal)
â”‚   â”œâ”€â”€ services.py        # ChatService with tool calling
â”‚   â”œâ”€â”€ rag_engine.py      # Hybrid RAG (cosine + BM25 + reranking)
â”‚   â”œâ”€â”€ prompts.py         # System prompts & tool definitions
â”‚   â”œâ”€â”€ config.py          # Configuration & environment
â”‚   â”œâ”€â”€ models.py          # Pydantic models
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ helper-scripts/
â”‚   â”‚   â”œâ”€â”€ index_chat.py           # Index chat â†’ FAISS
â”‚   â”‚   â”œâ”€â”€ chat_to_user_persona.py # Generate persona
â”‚   â”‚   â””â”€â”€ index_to_json.py        # Export to JSON
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ CHAT.txt
â”‚   â”‚   â”œâ”€â”€ user-persona.md
â”‚   â”‚   â”œâ”€â”€ faiss_index.bin
â”‚   â”‚   â””â”€â”€ metadata.pkl
â”œâ”€â”€ frontend               # Nextjs frontend
â”œâ”€â”€ llm-docs/              # Cerebras & Groq API docs
â”œâ”€â”€ memari-docs/           # Project documentation
â”œâ”€â”€ streamlit/             # Prototype streamlit app
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- Node.js 18+ with Bun
- Cerebras API Key
- Groq API Key

### 1. Clone & Setup Backend

```bash
cd backend
pip install -r requirements.txt
```

### 2. Configure Environment

Create `backend/.env`:

```env
CEREBRAS_API_KEY=your_cerebras_key
GROQ_API_KEY=your_groq_key
```

### 3. Run Backend

```bash
cd backend
uvicorn main:app --reload
```

### 4. Run Frontend

```bash
cd frontend
bun install
bun dev
```

---

## ğŸ§ª Memory Pipeline

### Indexing Flow

```
CHAT.txt â†’ Session Chunking â†’ LLM Rewriting â†’ Embeddings â†’ FAISS + BM25
              ("Human 1: Hi")   (Cerebras)    (MiniLM)
```

### Retrieval Flow (Hybrid Search)

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  75% Cosine (FAISS) + 25% BM25     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
CrossEncoder Re-ranking (ms-marco-MiniLM)
    â†“
Top 5 Results â†’ LLM Context
```

### Chat Flow (Single API Call Pattern)

```
User â†’ Safety (LlamaGuard) â†’ LLM + Tools â†’ Execute â†’ Final JSON Response
```

---

## ğŸ’¡ Key Features

- **Session-Based Chunking**: Conversations are split by `"Human 1: Hi"` delimiter
- **LLM-Optimized Memories**: Raw chat is rewritten for better semantic retrieval
- **Dual Metadata Storage**: Both original and rewritten text preserved
- **Tool-Based Memory Access**: Memory is retrieved only when needed via function calling
- **Latency Optimization**: Dense retrieval first, fusion only when confidence is low

---

## ğŸ“Š Current Stats

| Metric | Value |
|--------|-------|
| Total Sessions | 93 |
| Total Characters | 53,149 |
| Estimated Tokens | ~13,255 |
| Embedding Dimension | 384 |
| Index Size | ~140 KB |

---

## ğŸ¨ Frontend Design

Inspired by Rumik AI's IRA interface:

| Pane | Content |
|------|---------|
| **Left (20%)** | Chat history database for reference |
| **Center (60%)** | Main chat interface with Ari |
| **Right (20%)** | Tools called, memory chunks, model reasoning |

---

## ğŸ›£ï¸ Roadmap

- [x] Session-based chat indexing
- [x] LLM-powered memory rewriting
- [x] FAISS vector storage
- [x] User persona generation
- [x] FastAPI backend with RAG pipeline
- [x] Hybrid search (75% cosine + 25% BM25)
- [x] CrossEncoder re-ranking
- [x] Fusion retrieval with query expansion
- [x] Tool calling integration (get_user_persona, get_long_term_memory)
- [x] Safety guardrails with LlamaGuard 4
- [x] Streamlit 3-pane frontend (prototype)
- [x] **Next.js production frontend at `/chat`**
- [x] **WhatsApp-style message bubbles**
- [x] **Tool use error recovery**
- [ ] Multi-user support
- [ ] Streaming responses
- [ ] Session persistence (database)

---

## ğŸ“„ License

This project is for demonstration and learning purposes and not to compete commercially with Rumik.ai. 

---

<p align="center">
  <strong>Built with â¤ï¸ by Akshansh for showcasing long-term AI memory</strong>
</p>
