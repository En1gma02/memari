<p align="center">
  <h1 align="center">ğŸ§  Memari</h1>
  <p align="center"><strong>An AI Chatbot with Infinite Long-Term Memory</strong></p>
  <p align="center">
    <em>Building human-like memory for AI conversations</em>
  </p>
</p>

---

## ğŸ¯ Vision

**Memari** is an AI chatbot named **Ari** designed to remember conversations like a human friend would. Unlike traditional chatbots that forget context after a session ends, Ari can recall references to events discussed weeks, months, or even years ago.

> *"Talking to Ari should feel like chatting with a friend on WhatsApp who actually remembers your life."*

This project demonstrates a novel **Hybrid Memory Architecture** for solving the infinite memory problem in conversational AI.

---

## ğŸ—ï¸ Architecture

Memari implements a **4-Layer Hybrid Memory System**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            MEMORY ARCHITECTUR                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  SHORT-TERM  â”‚  â”‚    USER     â”‚  â”‚  LONG-TERM   â”‚  â”‚   ARI'S      â”‚   â”‚
â”‚  â”‚  MEMORY      â”‚  â”‚   PERSONA   â”‚  â”‚   MEMORY     â”‚  â”‚   LIFE       â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚ Last  10 msgsâ”‚  â”‚ Structured  â”‚  â”‚ FAISS Vector â”‚  â”‚ FAISS Vector â”‚   â”‚
â”‚  â”‚ + session in â”‚  â”‚ MD file     â”‚  â”‚ DB: User     â”‚  â”‚ DB: Ari's    â”‚   â”‚
â”‚  â”‚ context      â”‚  â”‚ with user   â”‚  â”‚ chat history â”‚  â”‚ life story   â”‚   â”‚
â”‚  â”‚ window       â”‚  â”‚ facts       â”‚  â”‚ embeddings   â”‚  â”‚ (334 chunks) â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â–²                 â–²                  â–²                 â–²         â”‚
â”‚         â”‚                 â”‚                  â”‚                 â”‚         â”‚
â”‚    Always Active     Tool Call          Tool Call         Tool Call      â”‚
â”‚                   get_user_persona  get_long_term_memory get_self_info   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Layers

| Layer | Purpose | Storage | Retrieval |
|-------|---------|---------|-----------|
| **Short-Term** | Immediate conversation context | In-memory buffer | Always included |
| **User Persona** | Facts, preferences, personality | Markdown file | Tool call |
| **Long-Term** | Historical conversations | FAISS + Embeddings | RAG Pipeline |
| **Ari's Life** | Ari's background, experiences, personality | FAISS + Embeddings | RAG Pipeline |

---

## ğŸ”§ Tech Stack

| Component | Technology |
|-----------|------------|
| **Frontend** | Next.js 16 + React 19 + shadcn/ui + Tailwind v4 |
| **Backend** | FastAPI (Python) |
| **Vector DB** | FAISS (faiss-cpu) |
| **Embeddings** | sentence-transformers (`all-MiniLM-L6-v2`) |
| **Re-ranking** | CrossEncoder (`ms-marco-MiniLM-L6-v2`) |
| **LLM - Fast** | Cerebras (`llama-3.1-8b`) |
| **LLM - Smart** | Groq (`gpt-oss-120b` with tool calling) |
| **Safety** | Groq (`llama-guard-4-12b`) |

---

## ğŸ“ Project Structure

```
memari/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py            # FastAPI endpoints (minimal)
â”‚   â”œâ”€â”€ services.py        # ChatService with tool calling
â”‚   â”œâ”€â”€ rag_engine.py      # V2 Hybrid RAG (cosine + BM25, query expansion, reranking, adaptive K)
â”‚   â”œâ”€â”€ prompts.py         # System prompts & tool definitions
â”‚   â”œâ”€â”€ config.py          # Configuration & environment
â”‚   â”œâ”€â”€ models.py          # Pydantic models
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ helper-scripts/
â”‚   â”‚   â”œâ”€â”€ index_chat.py           # Index chat â†’ FAISS
â”‚   â”‚   â”œâ”€â”€ index_ari_life.py       # Index Ari's life â†’ FAISS
â”‚   â”‚   â”œâ”€â”€ chat_to_user_persona.py # Generate persona
â”‚   â”‚   â””â”€â”€ index_to_json.py        # Export to JSON
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ CHAT.txt
â”‚   â”‚   â”œâ”€â”€ user-persona.md
â”‚   â”‚   â”œâ”€â”€ ari-life.md             # Ari's life story
â”‚   â”‚   â”œâ”€â”€ faiss_index.bin         # User chat index
â”‚   â”‚   â”œâ”€â”€ faiss_index_ari.bin     # Ari's life index
â”‚   â”‚   â”œâ”€â”€ metadata.pkl
â”‚   â”‚   â””â”€â”€ metadata_ari.pkl
â”œâ”€â”€ benchmark/             # RAG benchmark suite
â”‚   â”œâ”€â”€ rag_benchmark.py           # V1 engine
â”‚   â”œâ”€â”€ rag_benchmark_v2.py        # V2 optimized engine
â”‚   â”œâ”€â”€ rag_benchmark_v3.py        # V3 experimental
â”‚   â”œâ”€â”€ run_benchmark.py           # Benchmark runner
â”‚   â”œâ”€â”€ metrics.py                 # Retrieval & QA metrics
â”‚   â”œâ”€â”€ generate_queries.py        # Test query generator
â”‚   â”œâ”€â”€ benchmarks.md              # Results & findings
â”‚   â””â”€â”€ results/                   # CSV & JSON outputs
â”œâ”€â”€ frontend/              # Next.js frontend
â”œâ”€â”€ llm-docs/              # Cerebras & Groq API docs
â”œâ”€â”€ memari-docs/           # Project documentation
â”œâ”€â”€ streamlit/             # Prototype streamlit app
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- Node.js 18+ with Bun
- Cerebras API Key
- Groq API Key

### 1. Clone & Setup Backend

```bash
cd backend
pip install -r requirements.txt
```

### 2. Configure Environment

Create `backend/.env`:

```env
CEREBRAS_API_KEY=your_cerebras_key
GROQ_API_KEY=your_groq_key
```

### 3. Run Backend

```bash
cd backend
uvicorn main:app --reload
```

### 4. Run Frontend

```bash
cd frontend
bun install
bun dev
```

---

## ğŸ§ª Memory Pipeline

### Indexing Flow

```
CHAT.txt â†’ Session Chunking â†’ LLM Rewriting â†’ Embeddings â†’ FAISS + BM25
              ("Human 1: Hi")   (Cerebras)    (MiniLM)
```

### Retrieval Flow (V2 Hybrid Search)

```
             User Query
                 â†“
         Query Expansion (LLM adds synonyms)
                 â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FAISS (50 candidates) â†’ BM25 (candidates)  â”‚
â”‚        75% Cosine + 25% BM25                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
      CrossEncoder Re-ranking (normalized)
                 â†“
         Adaptive Top-K (3/5/7)
                 â†“
Contextual Expansion (Â±1 neighbors, â‰¥25% only)
                 â†“
              LLM Context
```

### Chat Flow (Single API Call Pattern)

```
User â†’ Safety (LlamaGuard) â†’ LLM + Tools â†’ Execute â†’ Final JSON Response
```

---

## ğŸ’¡ Key Features

- **Session-Based Chunking**: Conversations are split by `"Human 1: Hi"` delimiter
- **LLM-Optimized Memories**: Raw chat is rewritten for better semantic retrieval
- **Dual Metadata Storage**: Both original and rewritten text preserved
- **Tool-Based Memory Access**: Memory is retrieved only when needed via function calling
- **V2 RAG Pipeline**: Query expansion + FAISS-first BM25 + Adaptive K + Contextual expansion
- **Parallel Fusion Retrieval**: Low-confidence queries trigger parallel multi-query search using ThreadPoolExecutor
- **Self-Awareness**: Ari has access to her own life story via `get_self_info` tool (334 indexed chunks)
- **Three-View Interface**: Seamlessly switch between Chat History, User Persona, and Ari's Life
- **Smart Safety Layer**: LlamaGuard flags are passed as warnings to the LLM (handles Hinglish false positives)
- **Zero-Shot Prompts**: Optimized prompts following Groq best practices (no example copying)
- **Identity Protection**: Ari never identifies as AI/OpenAI/GPT

---

## ğŸ“Š Current Stats

| Metric | Value |
|--------|-------|
| Total Sessions | 93 |
| Total Characters | 53,149 |
| Estimated Tokens | ~13,255 |
| Embedding Dimension | 384 |
| Index Size | ~140 KB |

---

## ğŸ¨ Frontend Design

Inspired by Rumik AI's IRA interface:

| Pane | Content |
|------|---------|
| **Left (20%)** | Dropdown selector: Chat History, User Persona, or Ari's Life with markdown rendering |
| **Center (60%)** | Main chat interface with Ari |
| **Right (20%)** | Memory Panel: Available tools, tools used, retrieved context chunks |

---

## ğŸ›£ï¸ Roadmap

- [x] Session-based chat indexing
- [x] LLM-powered memory rewriting
- [x] FAISS vector storage
- [x] User persona generation
- [x] FastAPI backend with RAG pipeline
- [x] Hybrid search (75% cosine + 25% BM25)
- [x] CrossEncoder re-ranking
- [x] Fusion retrieval with query expansion
- [x] Tool calling integration (get_user_persona, get_long_term_memory, get_self_info)
- [x] Safety guardrails with LlamaGuard 4
- [x] Streamlit 3-pane frontend (prototype)
- [x] **Next.js production frontend at `/chat`**
- [x] **WhatsApp-style message bubbles**
- [x] **Tool use error recovery**
- [x] **Ari's Life knowledge base (334 indexed chunks)**
- [x] **Custom dropdown selector with view descriptions**
- [x] **Markdown rendering for persona and life story**
- [x] **V2 RAG: Query expansion, FAISS-first BM25, Adaptive K**
- [x] **Contextual chunk expansion (Â±1 neighbors)**
- [x] **RAG Benchmark Suite (V1/V2/V3 comparison)**
- [x] **Zero-shot prompts (Groq best practices)**
- [x] **Parallel fusion retrieval (ThreadPoolExecutor)**
- [x] **Smart safety layer (warning injection vs blocking)**
- [x] **Identity protection (never identifies as AI)**
- [x] **Proactive tool usage (aggressive memory access)**
- [ ] Multi-user support
- [ ] Streaming responses
- [ ] Session persistence (database)

---

## ğŸ“„ License

This project is for demonstration and learning purposes and not to compete commercially with Rumik.ai. 

---

<p align="center">
  <strong>Built with â¤ï¸ by Akshansh for showcasing long-term AI memory</strong>
</p>
