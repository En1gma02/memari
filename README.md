<p align="center">
  <h1 align="center">ğŸ§  Memari</h1>
  <p align="center"><strong>An AI Chatbot with Infinite Long-Term Memory</strong></p>
  <p align="center">
    <em>Building human-like memory for AI conversations</em>
  </p>
</p>

---

## ğŸ¯ Vision

**Memari** is an AI chatbot named **Ari** designed to remember conversations like a human friend would. Unlike traditional chatbots that forget context after a session ends, Ari can recall references to events discussed weeks, months, or even years ago.

> *"Talking to Ari should feel like chatting with a friend on WhatsApp who actually remembers your life."*

This project demonstrates a novel **Hybrid Memory Architecture** for solving the infinite memory problem in conversational AI.

---

## ğŸ—ï¸ Architecture

Memari implements a **3-Layer Hybrid Memory System**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MEMORY ARCHITECTURE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  SHORT-TERM      â”‚  â”‚  USER PERSONA    â”‚  â”‚  LONG-TERM     â”‚ â”‚
â”‚  â”‚  MEMORY          â”‚  â”‚  MEMORY          â”‚  â”‚  MEMORY        â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Last 5-10 msgs   â”‚  â”‚ Structured MD    â”‚  â”‚ FAISS Vector   â”‚ â”‚
â”‚  â”‚ in context       â”‚  â”‚ file with user   â”‚  â”‚ Database with  â”‚ â”‚
â”‚  â”‚ window           â”‚  â”‚ facts, likes,    â”‚  â”‚ session-based  â”‚ â”‚
â”‚  â”‚                  â”‚  â”‚ personality      â”‚  â”‚ embeddings     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â–²                      â–²                     â–²          â”‚
â”‚         â”‚                      â”‚                     â”‚          â”‚
â”‚    Always Active          Tool Call            Tool Call        â”‚
â”‚                        get_user_persona    get_long_term_memory â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Memory Layers

| Layer | Purpose | Storage | Retrieval |
|-------|---------|---------|-----------|
| **Short-Term** | Immediate conversation context | In-memory buffer | Always included |
| **User Persona** | Facts, preferences, personality | Markdown file | Tool call |
| **Long-Term** | Historical conversations | FAISS + Embeddings | RAG Pipeline |

---

## ğŸ”§ Tech Stack

| Component | Technology |
|-----------|------------|
| **Frontend** | Next.js 15, Tailwind CSS, Shadcn UI |
| **Backend** | FastAPI (Python) |
| **Vector DB** | FAISS (faiss-cpu) |
| **Embeddings** | sentence-transformers (`all-MiniLM-L6-v2`) |
| **LLM - Fast** | Cerebras (`llama-3.1-8b`) |
| **LLM - Smart** | Groq (`gpt-oss-120b` with reasoning) |
| **Safety** | Groq (`llama-guard-4-12b`) |

---

## ğŸ“ Project Structure

```
memari/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ helper-scripts/
â”‚   â”‚   â”œâ”€â”€ index_chat.py       # Index chat history â†’ FAISS
â”‚   â”‚   â”œâ”€â”€ index_to_json.py    # Export index to JSON
â”‚   â”‚   â””â”€â”€ chat_index.json     # Viewable indexed chunks
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ ari-system-prompt.md    # Ari's personality prompt
â”‚   â”œâ”€â”€ ari-life.md             # Ari's persona/backstory
â”‚   â”œâ”€â”€ faiss_index.bin         # FAISS vector index
â”‚   â”œâ”€â”€ metadata.pkl            # Index metadata
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/                   # Next.js web app
â”œâ”€â”€ llm-docs/                   # Cerebras & Groq API docs
â”œâ”€â”€ memari-docs/                # Project documentation
â”œâ”€â”€ CHAT.txt                    # Sample chat history (30K tokens)
â”œâ”€â”€ IDEA_CONTEXT.md             # Project vision & architecture
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- Node.js 18+ with Bun
- Cerebras API Key
- Groq API Key

### 1. Clone & Setup Backend

```bash
cd backend
pip install -r requirements.txt
```

### 2. Configure Environment

Create `backend/.env`:

```env
CEREBRAS_API_KEY=your_cerebras_key
GROQ_API_KEY=your_groq_key
```

### 3. Index Chat History

```bash
cd backend/helper-scripts
python index_chat.py
```

This will:
1. Load `CHAT.txt` and split into sessions
2. Rewrite each session with LLaMA 3.1 8B for optimal retrieval
3. Generate embeddings with `all-MiniLM-L6-v2`
4. Save FAISS index and metadata

### 4. Run Backend

```bash
cd backend
uvicorn main:app --reload
```

### 5. Run Frontend

```bash
cd frontend
bun install
bun dev
```

---

## ğŸ§ª Long-Term Memory Pipeline

### Indexing Flow

```
CHAT.txt â†’ Session Chunking â†’ LLM Rewriting â†’ Embeddings â†’ FAISS Index
              ("Human 1: Hi")   (Cerebras)    (MiniLM)
```

### Retrieval Flow

```
User Query â†’ Query Expansion â†’ Hybrid Search â†’ Reranking â†’ Context Expansion
                (4 queries)    (Dense + BM25)              (neighboring chunks)
                    â†“
            Response Generation â† Safety Check â† LLM Response
                (Groq gpt-oss-120b)  (LlamaGuard)
```

---

## ğŸ’¡ Key Features

- **Session-Based Chunking**: Conversations are split by `"Human 1: Hi"` delimiter
- **LLM-Optimized Memories**: Raw chat is rewritten for better semantic retrieval
- **Dual Metadata Storage**: Both original and rewritten text preserved
- **Tool-Based Memory Access**: Memory is retrieved only when needed via function calling
- **Latency Optimization**: Dense retrieval first, fusion only when confidence is low

---

## ğŸ“Š Current Stats

| Metric | Value |
|--------|-------|
| Total Sessions | 93 |
| Total Characters | 53,149 |
| Estimated Tokens | ~13,255 |
| Embedding Dimension | 384 |
| Index Size | ~140 KB |

---

## ğŸ¨ Frontend Design

Inspired by Rumik AI's IRA interface:

| Pane | Content |
|------|---------|
| **Left (20%)** | Chat history database for reference |
| **Center (60%)** | Main chat interface with Ari |
| **Right (20%)** | Tools called, memory chunks, model reasoning |

---

## ğŸ›£ï¸ Roadmap

- [x] Session-based chat indexing
- [x] LLM-powered memory rewriting
- [x] FAISS vector storage
- [ ] FastAPI backend with RAG pipeline
- [ ] Fusion retrieval with query expansion
- [ ] Reranking algorithm
- [ ] Context expansion (neighboring chunks)
- [ ] User persona management
- [ ] Tool calling integration
- [ ] Next.js frontend
- [ ] Safety guardrails with LlamaGuard

---

## ğŸ“„ License

This project is for demonstration and learning purposes.

---

<p align="center">
  <strong>Built with â¤ï¸ by Akshansh for showcasing long-term AI memory</strong>
</p>
